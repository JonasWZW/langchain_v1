# -*- coding:utf-8 -*-
from langchain_core.tools import tool

from config import deepseek

model = deepseek


@tool
def get_weather(location: str) -> str:
    """Get the weather at a location."""
    return f"It's sunny in {location}."


def tool_execute_loop():
    # Bind (potentially multiple) tools to the model
    model_with_tools = model.bind_tools([get_weather])

    # Step 1: Model generates tool calls
    messages = [{"role": "user", "content": "What's the weather in Boston?"}]
    ai_msg = model_with_tools.invoke(messages)
    messages.append(ai_msg)

    # Step 2: Execute tools and collect results
    for tool_call in ai_msg.tool_calls:
        # Execute the tool with the generated arguments
        tool_result = get_weather.invoke(tool_call)
        messages.append(tool_result)

    # Step 3: Pass results back to model for final response
    final_response = model_with_tools.invoke(messages)
    print(final_response.text)
    # "The current weather in Boston is 72Â°F and sunny."


def parallel_tool_call():
    model_with_tools = model.bind_tools([get_weather])
    messages = [{"role": "user", "content": "What's the weather in Boston and Tokyo?"}]
    response = model_with_tools.invoke(
        messages
    )
    messages.append(response)
    # The model may generate multiple tool calls
    print(response.tool_calls)
    # [
    #   {'name': 'get_weather', 'args': {'location': 'Boston'}, 'id': 'call_1'},
    #   {'name': 'get_weather', 'args': {'location': 'Tokyo'}, 'id': 'call_2'},
    # ]

    # Execute all tools (can be done in parallel with async)
    results = []
    for tool_call in response.tool_calls:
        if tool_call['name'] == 'get_weather':
            result = get_weather.invoke(tool_call)
            results.append(result)
    messages.extend(results)
    res = model_with_tools.invoke(messages)
    print(res)


if __name__ == '__main__':
    parallel_tool_call()
